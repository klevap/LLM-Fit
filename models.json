[
  {
    "id": "llama3.1",
    "name": "Llama 3.1",
    "license": "Meta Llama",
    "features": ["Decoder-only", "RMSNorm", "SwiGLU", "RoPE", "GQA"],
    "notes": {
      "en": "Uses tiktoken tokenizer with a 128K vocabulary. GQA significantly saves KV cache memory. 8B and 70B variants are widely used.",
      "ru": "Использует токенайзер tiktoken со словарем 128K. GQA значительно экономит память KV кэша. Варианты 8B и 70B широко используются."
    },
    "links": [
      { "t": "Meta Blog", "href": "https://ai.meta.com/blog/meta-llama-3-1/" }
    ],
    "variants": [
      { "id": "8b", "name": "8B", "params": 8e9, "layers": 32, "hidden": 4096, "heads": 32, "kvHeads": 8, "ffnMult": 3.5, "vocab": 128256, "tieEmb": false, "ctx": 131072 },
      { "id": "70b", "name": "70B", "params": 70e9, "layers": 80, "hidden": 8192, "heads": 64, "kvHeads": 8, "ffnMult": 3.0, "vocab": 128256, "tieEmb": false, "ctx": 131072 },
      { "id": "405b", "name": "405B", "params": 405e9, "layers": 128, "hidden": 16384, "heads": 128, "kvHeads": 16, "ffnMult": 2.6, "vocab": 128256, "tieEmb": false, "ctx": 131072 }
    ]
  },
  {
    "id": "mixtral",
    "name": "Mixtral",
    "license": "Apache-2.0",
    "features": ["Decoder-only", "MoE Top-2", "RMSNorm", "SwiGLU", "RoPE", "GQA", "SWA"],
    "notes": {
        "en": "Sparse Mixture-of-Experts (MoE) model. Only 2 out of 8 experts are active per token, making inference faster than for a dense model of the same total size.",
        "ru": "Спарсная модель Mixture-of-Experts (MoE). Только 2 из 8 экспертов активны для каждого токена, что делает инференс быстрее, чем у плотной модели того же общего размера."
    },
    "links": [
      { "t": "HF Docs", "href": "https://huggingface.co/docs/transformers/model_doc/mixtral" }
    ],
    "variants": [
      { "id": "8x7b", "name": "8x7B", "params": 46.7e9, "layers": 32, "hidden": 4096, "heads": 32, "kvHeads": 8, "ffnMult": 2.66, "vocab": 32000, "tieEmb": true, "ctx": 32768, "moe": { "experts": 8, "topK": 2 } },
      { "id": "8x22b", "name": "8x22B", "params": 141e9, "layers": 56, "hidden": 6144, "heads": 48, "kvHeads": 8, "ffnMult": 2.66, "vocab": 32000, "tieEmb": true, "ctx": 65536, "moe": { "experts": 8, "topK": 2 } }
    ]
  },
  {
    "id": "qwen2",
    "name": "Qwen 2",
    "license": "Qianwen",
    "features": ["Decoder-only", "RMSNorm", "SwiGLU", "RoPE", "GQA"],
    "notes": {
        "en": "Strong multilingual capabilities and long context support. Uses GQA with specified KV heads.",
        "ru": "Высокие мультиязычные способности и поддержка длинных контекстов. Использует GQA с указанным числом KV голов."
    },
    "links": [
      { "t": "HF 72B", "href": "https://huggingface.co/Qwen/Qwen2-72B-Instruct" }
    ],
    "variants": [
      { "id": "7b", "name": "7B", "params": 7.6e9, "layers": 28, "hidden": 3584, "heads": 28, "kvHeads": 4, "ffnMult": 3.5, "vocab": 152064, "tieEmb": true, "ctx": 131072 },
      { "id": "72b", "name": "72B", "params": 72.7e9, "layers": 80, "hidden": 8192, "heads": 64, "kvHeads": 8, "ffnMult": 3.0, "vocab": 152064, "tieEmb": true, "ctx": 131072 }
    ]
  },
  {
    "id": "gemma2",
    "name": "Gemma 2",
    "license": "Gemma",
    "features": ["Decoder-only", "RMSNorm", "Gated MLP", "RoPE", "GQA"],
    "notes": {
        "en": "Google's family of lightweight, state-of-the-art open models. 9B and 27B variants offer improved performance and efficiency.",
        "ru": "Семейство легковесных и современных открытых моделей от Google. Варианты 9B и 27B предлагают улучшенную производительность и эффективность."
    },
    "links": [
      { "t": "Google Blog", "href": "https://developers.googleblog.com/gemma-explained-new-in-gemma-2" }
    ],
    "variants": [
      { "id": "9b", "name": "9B", "params": 9e9, "layers": 42, "hidden": 3584, "heads": 28, "kvHeads": 4, "ffnMult": 3.0, "vocab": 256000, "tieEmb": true, "ctx": 8192 },
      { "id": "27b", "name": "27B", "params": 27e9, "layers": 52, "hidden": 5120, "heads": 40, "kvHeads": 8, "ffnMult": 3.0, "vocab": 256000, "tieEmb": true, "ctx": 8192 }
    ]
  },
  {
    "id": "dbrx",
    "name": "DBRX",
    "license": "Databricks Open",
    "features": ["Decoder-only", "MoE fine-grained", "RMSNorm", "RoPE", "GQA"],
    "notes": {
        "en": "A 132B total parameter MoE model, with ~36B active parameters. It has 16 experts and selects 4 per token. Features a 32K context window.",
        "ru": "MoE модель с 132B общих параметров и ~36B активных. Имеет 16 экспертов и выбирает 4 на токен. Контекстное окно 32K."
    },
    "links": [
      { "t": "Databricks Blog", "href": "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm" }
    ],
    "variants": [
      { "id": "132b", "name": "132B (36B active)", "params": 132e9, "layers": 62, "hidden": 12288, "heads": 96, "kvHeads": 8, "ffnMult": 2.66, "vocab": 131072, "tieEmb": true, "ctx": 32768, "moe": { "experts": 16, "topK": 4 } }
    ]
  }
]