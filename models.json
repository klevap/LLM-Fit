[
  {
    "id": "llama3.1",
    "name": "Llama 3.1",
    "license": "Meta Llama",
    "notes": {
      "en": "Flagship dense models. 128K tokenizer vocab. GQA reduces KV cache.",
      "ru": "Флагманские плотные модели. Словарь токенайзера 128K. GQA снижает память KV кэша."
    },
    "links": [
      {
        "t": "Meta Blog",
        "href": "https://ai.meta.com/blog/meta-llama-3-1/"
      }
    ],
    "variants": [
      {
        "id": "8b",
        "name": "8B",
        "params": 8030000000.0,
        "layers": 32,
        "hidden": 4096,
        "heads": 32,
        "kvHeads": 8,
        "ffnMult": 3.5,
        "vocab": 128256,
        "tieEmb": false,
        "ctx": 131072,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA"
      },
      {
        "id": "70b",
        "name": "70B",
        "params": 70600000000.0,
        "layers": 80,
        "hidden": 8192,
        "heads": 64,
        "kvHeads": 8,
        "ffnMult": 3.0,
        "vocab": 128256,
        "tieEmb": false,
        "ctx": 131072,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA"
      },
      {
        "id": "405b",
        "name": "405B",
        "params": 405000000000.0,
        "layers": 126,
        "hidden": 16384,
        "heads": 128,
        "kvHeads": 8,
        "ffnMult": 3.25,
        "vocab": 128256,
        "tieEmb": false,
        "ctx": 131072,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA"
      }
    ]
  },
  {
    "id": "qwen",
    "name": "Qwen",
    "license": "Qianwen",
    "notes": {
      "en": "Strong multilingual capabilities and long context support. Uses GQA.",
      "ru": "Высокие мультиязычные способности и поддержка длинных контекстов. Использует GQA."
    },
    "links": [
      {
        "t": "Qwen2 Blog",
        "href": "https://qwenlm.github.io/blog/qwen2/"
      },
      {
        "t": "Qwen 3 Info",
        "href": "https://zhuanlan.zhihu.com/p/711329623"
      }
    ],
    "variants": [
      {
        "id": "qwen2-7b",
        "name": "Qwen 2 7B",
        "params": 7600000000.0,
        "layers": 28,
        "hidden": 3584,
        "heads": 28,
        "kvHeads": 4,
        "ffnMult": 3.5,
        "vocab": 152064,
        "tieEmb": true,
        "ctx": 131072,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA"
      },
      {
        "id": "qwen2-72b",
        "name": "Qwen 2 72B",
        "params": 72700000000.0,
        "layers": 80,
        "hidden": 8192,
        "heads": 64,
        "kvHeads": 8,
        "ffnMult": 3.0,
        "vocab": 152064,
        "tieEmb": true,
        "ctx": 131072,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA"
      },
      {
        "id": "qwen3-235b",
        "name": "Qwen 3 235B",
        "params": 235000000000.0,
        "activeParams": 22000000000.0,
        "layers": 94,
        "hidden": 8192,
        "heads": 64,
        "kvHeads": 16,
        "ffnMult": 3.5,
        "vocab": 152064,
        "tieEmb": true,
        "ctx": 128000,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA",
        "moe": {
          "experts_total": 128,
          "topK": 8
        }
      }
    ]
  },
  {
    "id": "gemma2",
    "name": "Gemma 2",
    "license": "Gemma",
    "notes": {
      "en": "Google open models with GQA and GeGLU activation.",
      "ru": "Открытые модели Google с GQA и GeGLU."
    },
    "links": [
      {
        "t": "Google Blog",
        "href": "https://blog.google/technology/developers/google-gemma-2/"
      }
    ],
    "variants": [
      {
        "id": "9b",
        "name": "9B",
        "params": 9000000000.0,
        "layers": 42,
        "hidden": 3584,
        "heads": 16,
        "kvHeads": 8,
        "ffnMult": 4.0,
        "vocab": 256000,
        "tieEmb": true,
        "ctx": 8192,
        "norm": "RMSNorm",
        "activation": "GeGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA"
      },
      {
        "id": "27b",
        "name": "27B",
        "params": 27000000000.0,
        "layers": 46,
        "hidden": 4608,
        "heads": 32,
        "kvHeads": 16,
        "ffnMult": 8.0,
        "vocab": 256000,
        "tieEmb": true,
        "ctx": 8192,
        "norm": "RMSNorm",
        "activation": "GeGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA"
      }
    ]
  },
  {
    "id": "mixtral",
    "name": "Mixtral",
    "license": "Apache 2.0",
    "notes": {
      "en": "Sparse Mixture-of-Experts models that activate top-K of total experts per MLP.",
      "ru": "Спарсные MoE модели, активируют top-K из всех экспертов в каждом MLP."
    },
    "links": [
      {
        "t": "Mistral 8x7B",
        "href": "https://mistral.ai/news/mixtral-of-experts"
      },
      {
        "t": "Mistral 8x22B",
        "href": "https://mistral.ai/news/mixtral-8x22b"
      }
    ],
    "variants": [
      {
        "id": "8x7b",
        "name": "8x7B",
        "params": 46700000000.0,
        "activeParams": 12900000000.0,
        "layers": 32,
        "hidden": 4096,
        "heads": 32,
        "kvHeads": 8,
        "ffnMult": 2.66,
        "vocab": 32000,
        "tieEmb": true,
        "ctx": 32768,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA",
        "moe": {
          "experts_total": 8,
          "topK": 2
        }
      },
      {
        "id": "8x22b",
        "name": "8x22B",
        "params": 141000000000.0,
        "activeParams": 39000000000.0,
        "layers": 56,
        "hidden": 6144,
        "heads": 48,
        "kvHeads": 8,
        "ffnMult": 2.66,
        "vocab": 32000,
        "tieEmb": true,
        "ctx": 65536,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA",
        "moe": {
          "experts_total": 8,
          "topK": 2
        }
      }
    ]
  },
  {
    "id": "deepseek",
    "name": "DeepSeek",
    "license": "DeepSeek/MIT",
    "notes": {
      "en": "MoE models with MLA attention. Use provided total and active parameters for accuracy.",
      "ru": "MoE модели с MLA. Для точности используем заданные total и active параметры."
    },
    "links": [
      {
        "t": "DeepSeek V2 Paper",
        "href": "https://arxiv.org/abs/2405.04434"
      }
    ],
    "variants": [
      {
        "id": "v2-236b",
        "name": "V2 236B",
        "params": 236000000000.0,
        "activeParams": 21000000000.0,
        "layers": 60,
        "hidden": 5120,
        "heads": 128,
        "kvHeads": 0,
        "ffnMult": 3.0,
        "vocab": 100000,
        "tieEmb": true,
        "ctx": 128000,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "MLA",
        "kvDimMLA": 512,
        "mlaRopeKV": 64,
        "moe": {
          "experts_total": 162,
          "shared": 2,
          "topK": 6
        }
      },
      {
        "id": "r1-671b",
        "name": "R1 671B",
        "params": 671000000000.0,
        "activeParams": 37000000000.0,
        "layers": null,
        "hidden": null,
        "heads": null,
        "kvHeads": null,
        "ffnMult": null,
        "vocab": 128000,
        "tieEmb": true,
        "ctx": 128000,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "MLA",
        "moe": {
          "experts_total": 257,
          "shared": 1,
          "topK": 8
        },
        "paramsOnly": true,
        "notes": {
          "en": "Public docs give total and active params plus expert counts. Architecture details such as layers and hidden size are not public, so the calculator will use the provided param counts.",
          "ru": "Публичные источники дают общий и активный размер, а также число экспертов. Детали архитектуры не раскрыты, калькулятор будет использовать заданные параметры."
        }
      }
    ]
  },
  {
    "id": "kimi",
    "name": "Kimi K2",
    "license": "MIT",
    "notes": {
      "en": "Trillion-parameter MoE focused on tools, agents, and coding.",
      "ru": "Триллионная MoE модель, фокус на инструментах, агентах и кодинге."
    },
    "links": [
      {
        "t": "Kimi K2 Model Card",
        "href": "https://huggingface.co/moonshotai/Kimi-K2-Instruct"
      }
    ],
    "variants": [
      {
        "id": "1t",
        "name": "1T",
        "params": 1000000000000.0,
        "activeParams": 32000000000.0,
        "layers": null,
        "hidden": null,
        "heads": 64,
        "kvHeads": null,
        "ffnMult": null,
        "vocab": 128000,
        "tieEmb": true,
        "ctx": 128000,
        "norm": "RMSNorm",
        "activation": "GeGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "MLA",
        "moe": {
          "experts_total": 384,
          "shared": 1,
          "topK": 8
        },
        "paramsOnly": true,
        "notes": {
          "en": "Public docs specify 1T total and 32B active parameters, 384 experts with top-8 routing plus one shared expert.",
          "ru": "По публичным данным 1T total и 32B active, 384 эксперта с top-8 и один общий эксперт."
        }
      }
    ]
  },
  {
    "id": "gpt-oss",
    "name": "GPT-OSS",
    "license": "Apache 2.0",
    "notes": {
      "en": "OpenAI open-weight MoE models optimized for reasoning and tool use.",
      "ru": "Открытые веса от OpenAI, MoE, оптимизированы под логику и инструменты."
    },
    "links": [
      {
        "t": "Intro post",
        "href": "https://openai.com/index/introducing-gpt-oss/"
      }
    ],
    "variants": [
      {
        "id": "21b",
        "name": "20B",
        "params": 21000000000.0,
        "activeParams": 3600000000.0,
        "layers": 24,
        "hidden": null,
        "heads": null,
        "kvHeads": null,
        "ffnMult": null,
        "vocab": 128000,
        "tieEmb": true,
        "ctx": 128000,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA",
        "moe": {
          "experts_total": 32,
          "topK": 4
        },
        "paramsOnly": true
      },
      {
        "id": "117b",
        "name": "120B",
        "params": 117000000000.0,
        "activeParams": 5100000000.0,
        "layers": 36,
        "hidden": null,
        "heads": null,
        "kvHeads": null,
        "ffnMult": null,
        "vocab": 128000,
        "tieEmb": true,
        "ctx": 128000,
        "norm": "RMSNorm",
        "activation": "SwiGLU",
        "mlp": "GatedMLP",
        "pos_embedding": "RoPE",
        "attention": "GQA",
        "moe": {
          "experts_total": 128,
          "topK": 4
        },
        "paramsOnly": true
      }
    ]
  }
]